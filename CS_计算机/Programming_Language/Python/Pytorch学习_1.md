---
title: Pytorch学习_1
mathjax: true
categories:
  - CS_计算机
  - Programming_Language
  - Python
date: 2023-07-18
abbrlink: 6c7f2ed0
---

# Pytorch学习_1
也是正式的开始学pytorch了，准备充分，炼丹去，前面的部分主要就是写一些基础的东西

<!--more-->

## Python中的切片
关于numpy中n维数组的表示方法
- `:`表示一整行
- `a:b`表示`a<=x<b`的元素，也就是说`1:3`表示第二、三个
- `0`表示第一个元素，这和c/c++都一样
- `-1`表示最后一个元素

## 关于tensor单独维度的计算
```python
# 单独维度求和示例程序
import torch

a = torch.rand(2, 3, 4, 5)
print(a)
print(torch.sum(a, dim=2))
# dim即在其余指标固定情况下，第n个数字指标求和相消
#体现在多维数组上即从外向内数相同的第n层，不跨越中括号的相同子tensor对应元素完全相加
```
该部分的某次运行结果如下
<img src="/images/Pytorch学习_1_图1.png" width="70%" height="70%">

## 记录一些简单的函数
```python
x = torch.arrange(12)
# 生成一维行向量，从1到11 
torch.randn(3, 4)
# 创建⼀个形状为(3,4)的张量。每个元素都从均值为0、标准差为1的标准⾼斯分布（正态分布）中随机采样
x = torch.normal(means, std, out=None)
# 返回一个张量，包含从给定参数means，std的离散正态分布中抽取随机数
# 均值means是一个张量，包含每个输出元素相关的正态分布的均值
# 标准差std是一个张量，包含每个输出元素相关的正态分布的标准差
# 均值和标准差的形状不须匹配，但每个张量的元素个数须相同
x.reshape(3, 4)
# 将矩阵变成3行4列的矩阵，即重排
# -1代表无此列
```

## 神经网络训练基础
在介绍深度神经网络之前，我们需要了解神经网络训练的基础知识。接下来奖介绍神经网络训练的整个过程，包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型

### 线性回归
回归是能为一个或多个自变量和因变量之间关系建模的一类方法。在自然科学和社科领域，回归经常用来表示输入和输出之间的关系

**线性回归的基本元素**如下：
为了开发模型，我们需要收集一个真实的数据集，在机器学习的术语中，该数据集称为**训练数据集**或**训练集**
其中的每行数据被称为**样本**或**数据点**或**数据样本**
我们把试图预测的目标称为**标签**或**目标**
预测所依据的自变量称为**特征**或**协变量**

### 线性模型
**线性假设**是指目标可以表示为特征的加权和，其中加权和的系数称为**权重**，后面跟和系数称为**偏置**、**偏移量**或**截距**
因为观测误差，所以即使确信特征与标签的潜在关系是线性的，也会加入一个噪声项来考虑观测误差带来的影响

### 损失函数
在考虑如何用模型拟合数据之前，我们需要确定一个拟合程度的度量。损失函数能够量化目标的实际值与预测值之间的差距
通常会使用非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0
回归问题中最常用的损失函数时平方误差函数
当样本$i$的预测值为$\hat{y}^{(i)}$，其相应的真实标签为$y^{(i)}$时，平方误差可以定义为以下公式
$$
l^{(i)}(\mathbf{w},b)=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2
$$
其中常数$\frac{1}{2}$不会造成本质的差别，只是在形式上简单一些。由于训练数据集并不受我们控制，使用经验误差只是关于模型参数的函数
由于平方误差函数中的二次方项，估计值和观测值之间较大的差异将导致更大的损失，为了度量模型在整个数据集上的质量，我们需要计算训练集n个样本上的损失均值(也等价于求和)
$$
L(\mathbf{w},b)=\frac{1}{n}\sum^{n}_{i=1}l^{(i)}(\mathbf{w},b)=\frac{1}{n}\sum^{n}_{i=1}\frac{1}{2}(\mathbf{w}^T\mathbf{x}^{(i)}+b-y^{(i)})^2
$$
在训练模型时，我们希望寻找一组参数$(\mathbf{w}^*,b^*)$，这组参数能最小化在所有训练样本上的总损失。如下
$$
\mathbf{w}^*,b^*=\sideset{}{}{\text{argmin }}_{\mathbf{w},b} L(\mathbf{w},b)
$$
通过计算可以证明，这里使用的最小化均方误差法等价于对线性模型的极大似然估计
> 在学到这里的时候，我想到了NFL定义(No Free Lunch Theoren，没有免费的午餐定理)，实际上关系不大，NFL定理表述的是对于特定的问题，我们比较不同模型的才有意义

### 解析解
线性回归的解可以用一个公式简单地表达表达出来，这类解被称为解析解(analytical solution)，但并不是所有的问题都存在解析解，事实上大部分问题都不会存在解析解，解析解无法广泛应用在深度学习中

### 随机梯度下降
利用梯度下降法可以几乎优化所有深度学习，他通过不断地在损失函数递减的方向上更新参数来降低误差，具体的内容这里不进行展开

### 用模型进行预测
给定已经完成学习的模型，输入特征估计目标的这一过程，在深度学习中的标准术语中被称为**预测**或**推断**